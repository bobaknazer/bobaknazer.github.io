<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<link href='http://fonts.googleapis.com/css?family=Hind:400,700' rel='stylesheet' type='text/css'>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<script
src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
type="text/javascript">
</script>
<link rel="stylesheet" href="ihp.css" type="text/css" />
<title>Nexus of Information and Computation Theories  Inference Problems Theme  March 7 - 18, 2016</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Nexus</div>
<div class="menu-item"><a href="index.html">General&nbsp;Info</a></div>
<div class="menu-item"><a href="participate.html"><b>How&nbsp;to&nbsp;Participate</b></a></div>
<div class="menu-item"><a href="organization.html">Scientific&nbsp;Committee</a></div>
<div class="menu-item"><a href="background.html">Scientific&nbsp;Background</a></div>
<div class="menu-category">Jan 25 - 29</div>
<div class="menu-item"><a href="tutorialabout.html">CIRM&nbsp;Tutorial&nbsp;Week</a></div>
<div class="menu-category">Feb 1 - 12</div>
<div class="menu-item"><a href="distcomp.html">Distributed&nbsp;Computation</a></div>
<div class="menu-category">Feb 15 - 26</div>
<div class="menu-item"><a href="inequalities.html">Fundamental&nbsp;Inequalities</a></div>
<div class="menu-category">Feb 29 - Mar 4</div>
<div class="menu-item"><a href="workshopabout.html">NICT&nbsp;Central&nbsp;Workshop</a></div>
<div class="menu-category">Mar 7 - 18</div>
<div class="menu-item"><a href="inference.html" class="current">Inference&nbsp;Problems</a></div>
<div class="menu-category">Mar 21 - Apr 1</div>
<div class="menu-item"><a href="secrecy.html">Secrecy&nbsp;and&nbsp;Privacy</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Nexus of Information and Computation Theories <br /> Inference Problems Theme <br /> March 7 - 18, 2016</h1>
</div>
<h3>Tentative Titles and Abstracts</h3>
<p><a name="Andoni"> </a></p>
<div class="infoblock">
<div class="blockcontent">
<p><i>Sketching and Embeddings</i> <br /> <b>Alex Andoni</b> (Columbia)<br /><br /> Abstract: Sketching for distance estimation is the problem where we need to design a possibly randomized function \(f\) from a metric space to short strings, such that from \(f(x)\) and \(f(y)\) we can estimate the distance between \(x\) and \(y\). This problem is a core problem in both the streaming and nearest neighbor search areas. We will discuss this problem and its connections to the theory of metric embeddings. In particular, we will discuss when and why sketching is equivalent to embedding into normed space such as \(\ell_1\).</p>
</div></div>
<p><a name="Braverman"> </a></p>
<div class="infoblock">
<div class="blockcontent">
<p><i>Beating CountSketch for Heavy Hitters in Insertion Streams</i> <br /> <b>Vladimir  Braverman</b> (Johns Hopkins University)<br /><br /> Abstract: Given a stream \(p_1, \ldots, p_m\) of items from a universe \(\mathcal{U}\), which, without loss of generality we identify with the set of integers \(\{1, 2, \ldots, n\}\), we consider the problem of returning all \(\ell_2\)-heavy hitters, i.e., those items \(j\) for which \(f_j \geq \epsilon \sqrt{F_2}\), where \(f_j\) is the number of occurrences of item \(j\) in the stream, and \(F_2 = \sum_{i \in [n]} f_i^2\). Such a guarantee is considerably stronger than the \(\ell_1\)-guarantee, which finds those \(j\) for which \(f_j \geq \epsilon m\). In 2002, Charikar, Chen, and Farach-Colton suggested the \(\mathsf{CountSketch}\) data structure, which finds all such \(j\) using \(\Theta(\log^2 n)\) bits of space (for constant \(\epsilon &gt; 0\)). The only known lower bound is \(\Omega(\log n)\) bits, which comes from the need to specify the identities of the items found.</p>
<p>In this paper, we show it is possible to achieve \(O(\log n (\log \log n)^2)\) bits for this problem. Our techniques, based on Gaussian processes, lead to a number of other new results for data streams, including</p>
<p>(1) The first algorithm for estimating \(F_2\) simultaneously at all points in a stream using only \(O(\log n\log\log n)\), for any constant \(v&gt;0\), bits of space, improving a natural union bound, and the algorithm of Huang, Tai, and Yi (2014).</p>
<p>(2) A way to estimate the \(\ell_{\infty}\) norm of a stream up to additive error \(\epsilon \sqrt{F_2}\) with \(O(\log n)\) bits of space, resolving Open Question 3 from the IITK 2006 list.</p>
</div></div>
<p><a name="Bresler"> </a></p>
<div class="infoblock">
<div class="blockcontent">
<p><i>TBA</i> <br />
<b>Guy Bresler</b> (Massachusetts Institute of Technology)</p>
</div></div>
<p><a name="Chakrabarti"> </a></p>
<div class="infoblock">
<div class="blockcontent">
<p><i>Overview of Communication Complexity</i> <br /> <b>Amit Chakrabarti</b> (Dartmouth College)<br /><br /> Abstract: This will be a tutorial-style (long) talk, giving an overview of the important basic results in communication complexity, with emphasis on results that can be seen (sometimes creatively) as designing efficient protocols for certain tasks. As we shall see, a number of lower bounds in communication complexity are also ultimately based on designing protocols.</p>
</div></div>
<p><a name="Cormode"> </a></p>
<div class="infoblock">
<div class="blockcontent">
<p><i>Compact summaries over large datasets</i> <br /> <b>Graham Cormode</b> (University of Warwick)<br /><br /> Abstract: A fundamental challenge in processing the massive quantities of information generated by modern applications is in extracting suitable representations of the data that can be stored, manipulated and interrogated on a single machine. A promising approach is in the design and analysis of compact summaries: data structures which capture key features of the data, and which can be created effectively over distributed data sets. Popular summary structures include the count distinct algorithms, which compactly approximate item set cardinalities, and sketches which allow vector norms and products to be estimated. These are very attractive, since they can be computed in parallel and combined to yield a single, compact summary of the data. This talk introduces the concepts and examples of compact summaries as well as some recent developments.</p>
</div></div>
<p><a name="Gamarnik"> </a></p>
<div class="infoblock">
<div class="blockcontent">
<p><i>TBA</i> <br />
<b>David Gamarnik</b> (Massachusetts Institute of Technology)</p>
</div></div>
<p><a name="Jaggi"> </a></p>
<div class="infoblock">
<div class="blockcontent">
<p><i>TBA</i> <br />
<b>Sidharth Jaggi</b> (The Chinese University of Hong Kong)</p>
</div></div>
<p><a name="Kapralov"> </a></p>
<div class="infoblock">
<div class="blockcontent">
<p><i>Streaming Lower Bounds for Approximating MAX-CUT</i> <br /> <b>Michael Kapralov</b> (EPFL)<br /><br /> Abstract: We consider the problem of estimating the value of max cut in a graph in the streaming model of computation. At one extreme, there is a trivial \(2\)-approximation for this problem that uses only \(O(\log n)\) space, namely, count the number of edges and output half of this value as the estimate for max cut value. On the other extreme, if one allows \(\tilde{O}(n)\) space, then a near-optimal solution to the max cut value can be obtained by storing an \(\tilde{O}(n)\)-size sparsifier that essentially preserves the max cut. An intriguing question is if poly-logarithmic space suffices to obtain a non-trivial approximation to the max-cut value (that is, beating the factor 2). It was recently shown that the problem of estimating the size of a maximum matching in a graph admits a non-trivial approximation in poly-logarithmic space.</p>
<p>Our main result is that any streaming algorithm that breaks the 2-approximation barrier requires \(\tilde{\Omega}(\sqrt{n})\) space even if the edges of the input graph are presented in random order. Our result is obtained by exhibiting a distribution over graphs which are either bipartite or \(\frac{1}{2}\)-far from being bipartite, and establishing that \(\tilde{\Omega}(\sqrt{n})\) space is necessary to differentiate between these two cases. Thus as a direct corollary we obtain that \(\tilde{\Omega}(\sqrt{n})\) space is also necessary to test if a graph is bipartite or \(\frac{1}{2}\)-far from being bipartite. We also show that for any \(\epsilon &gt; 0\), any streaming algorithm that obtains a \((1 + \epsilon)\)-approximation to the max cut value when edges arrive in  adversarial order requires \(n^{1 - O(\epsilon)}\) space, implying that \(\Omega(n)\) space is necessary to obtain an arbitrarily good approximation to the max cut value.</p>
</div></div>
<p><a name="Konrad"> </a></p>
<div class="infoblock">
<div class="blockcontent">
<p><i>Challenges in Streaming XML</i> <br /> <b>Christian Konrad</b> (Reykjavik University)<br /><br /> Abstract: Today, XML (eXtensible Markup Language) is ubiquitous. For example, it is the standard file format for data exchange on the Internet, and (often massive) XML databases are widely employed. Streaming XML, i.e., the processing of XML streams, gained popularity in recent years. In many applications, streaming processing is simply the only option (e.g. when monitoring data in sensor networks), but also in application where more involved approaches are possible, streaming algorithms often outperform usual non-streaming approaches.</p>
<p>In this presentation, we discuss some of the challenges that arise when processing streaming XML. We discuss streaming algorithms for fundamental XML-related problems such as well-formedness and validity of XML documents. Presented techniques include hashing, randomization and communication complexity.</p>
</div></div>
<p><a name="Kontoyiannis"> </a></p>
<div class="infoblock">
<div class="blockcontent">
<p><i>Testing temporal causality and estimating directed information</i> <br /> <b>Ioannis Kontoyiannis</b> (Athens U of Econ &amp; Business)<br /><br /> Abstract: The problem of estimating the directed information rate between two Markov chains of arbitrary (but finite) order is considered. Specifically for the so-called &ldquo;plug-in&rdquo; (or maximum-likelihood) estimator, under natural conditions we show that it is consistent with probability one, and that it is asymptotically Gaussian. From this it is show that its convergence rate is of \(O(1/\sqrt{n})\), which is the best possible. A connection is established between this estimation problem and that of performing a hypothesis test for the presence of causal influence between the two processes. Under the null hypothesis, which corresponds to the absence of (temporal) causality, we show that the plug-in estimator has an asymptotic \(\chi^2\) distribution, and that this estimator can be expressed precisely in terms of the classical likelihood ratio statistic. Combining these two results facilitates the design of a Neyman-Pearson likelihood ratio test for the presence of causal influence.</p>
<p>This is joint work with Maria Skoularidou.</p>
</div></div>
<p><a name="Lu"> </a></p>
<div class="infoblock">
<div class="blockcontent">
<p><i>Dynamics of Randomized Iterative Methods for Large-Scale Inference Problems</i> <br /> <b>Yue Lu</b> (Harvard University)<br /><br /> Abstract: In this talk, I will present an exact analysis of the dynamics of randomized iterative methods for solving inference problems. I will show that, in the large systems limit, the dynamics of these algorithms converges to trajectories governed by a set of deterministic and coupled ODEs or PDEs. Analyzing these deterministic ODEs and PDEs allows one to establish performance guarantees of the associated randomized iterative algorithms.</p>
</div></div>
<p><a name="Macris"> </a></p>
<div class="infoblock">
<div class="blockcontent">
<p><i>Spatial coupling as a proof technique</i> <br /> <b>Nicolas Macris</b> (EPFL)<br /><br /> Abstract: This talk will outline a recent set of ideas on using spatially coupled ensembles to deduce properties of the underlying non-coupled ensemble. An application is a proof of the replica symmetric formula for conditionnal entropy of Low-Density-Parity-Check codes on arbitrary binary input memoryless channels, as well as a proof of the Maxwell area construction for such systems. Applications to lossy source coding and satisfiability will be discussed time permitting.</p>
</div></div>
<p><a name="McGregor"> </a></p>
<div class="infoblock">
<div class="blockcontent">
<p><i>Graph Sketching and Streaming Tutorial</i> <br /> <b>Andrew McGregor</b> (University of Massachusetts)<br /><br /> Abstract: We'll present a 3 hour tutorial covering recent algorithmic results on processing massive graphs via random linear projections, aka sketches, and data streams.</p>
</div></div>
<p><a name="Molkaraie"> </a></p>
<div class="infoblock">
<div class="blockcontent">
<p><i>Efficient Monte Carlo Methods for the Potts Model at Low Temperature</i> <br /> <b>Mehdi Molkaraie</b> (UPF)<br /><br /> Abstract: We consider the problem of estimating the partition function of the ferromagnetic q-state Potts model. We propose an importance sampling algorithm in the dual of the normal factor graph representing the model. The algorithm can efficiently compute an estimate of the partition function when the coupling parameters of the model are strong (corresponding to models at low temperature) or when the model contains a mixture of strong and weak couplings. We show that, in this setting, the proposed algorithm significantly outperforms the state-of-the-art methods.</p>
</div></div>
<p><a name="Monemizadeh"> </a></p>
<div class="infoblock">
<div class="blockcontent">
<p><i>Sliding Windows and Clustering Problems</i> <br /> <b>Morteza Monemizadeh</b> (Charles University)<br /><br /> Abstract: We explore clustering problems in the streaming sliding window model in both general metric spaces and Euclidean space. We present the first polylogarithmic space \(O(1)\)-approximation to the metric \(k\)-median and metric \(k\)-means problems in the sliding window model, answering the main open problem posed by Babcock, Datar, Motwani and O'Callaghan, which has remained unanswered for over a decade. Our algorithm uses \(O(k^3 \log^6 n)\) space and \(\mathrm{poly}(k, \log n)\) update time. This is an exponential improvement on the space required by the technique due to Babcock, et al. We introduce a data structure that extends smooth histograms as introduced by Braverman and Ostrovsky to operate on a broader class of functions. In particular, we show that using only polylogarithmic space we can maintain a summary of the current window from which we can construct an \(O(1)\)-approximate clustering solution. </p>
<p>Merge-and-reduce is a generic method in computational geometry for adapting offline algorithms to the insertion-only streaming model. Several well-known coreset constructions are maintainable in the insertion-only streaming model using this method, including well-known coreset techniques for the \(k\)-median and \(k\)-means problems in both low-and high-dimensional Euclidean space. Previous work has adapted coreset techniques to the insertion-deletion model, but translating them to the sliding window model has remained a challenge. We give the first algorithm that, given an insertion-only streaming coreset of space \(s\) (maintained using merge-and-reduce method), maintains this coreset in the sliding window model using \(O(s^2\epsilon^{-2}\log n)\) space.</p>
<p>For clustering problems, our results constitute the first significant step towards resolving problem number 20 from the List of Open Problems in Sublinear Algorithms.</p>
</div></div>
<p><a name="Nelson"> </a></p>
<div class="infoblock">
<div class="blockcontent">
<p><i>Optimal approximate matrix product in terms of stable rank</i> <br /> <b>Jelani Nelson</b> (Harvard)<br /><br /> Abstract: We give two different proofs that use the subspace embedding guarantee in a black box way to show that one can achieve the spectral norm guarantee for approximate matrix multiplication with a dimensionality-reducing map that has \(O(r&nbsp; / \epsilon^2)\) rows, where \(r\) is the maximum stable rank of the two matrices being multiplied. This resolves the main open questions of (Magen, Zouzias SODA&rsquo;11) and (Kyrillidis, Vlachos, Zouzias ISIT&rsquo;14).</p>
<p>Our work has already been applied by (Cohen et al, STOC&rsquo;15) to obtain the new results on dimensionality reduction for k-means clustering, and can also be applied to arguments of (Yang, Pilanci, Wainwright&rsquo;15) to yield new dimensionality reduction results for non-parametric regression. We also show some new implications for least squares regression and low-rank approximation.</p>
<p>This talk is based on joint work with Michael B. Cohen (MIT) and David P. Woodruff (IBM Almaden).</p>
</div></div>
<p><a name="Oh"> </a></p>
<div class="infoblock">
<div class="blockcontent">
<p><i>Near-optimal message-passing algorithms for crowdsourcing</i> <br /> <b>Sewoong Oh</b> (UIUC)<br /><br /> Abstract: Crowdsourcing systems, like Amazon Mechanical Turk, provide platforms where large-scale projects are broken into small tasks that are electronically distributed to numerous on-demand contributors. Because these low-paid workers can be unreliable, we need to devise schemes to increase confidence in our answers, typically by assigning each task multiple times and combining the answers in some way. I will present a rigorous treatment of this problem, and provide both an optimal task assignment scheme (using a random graph) and an optimal inference algorithm (based on low-rank matrix approximation and belief propagation) for that task assignment.</p>
<p>We represent crowdsourcing systems using graphical models and address the problem of inference in this graphical model. Standard techniques like belief propagation are difficult to implement in practice because they require knowledge of a priori distribution of the problem parameters. Instead, we propose a message-passing algorithm that does not require any knowledge of the apriori distributions. We show that this algorithm achieves performance close to a minimax lower bound. To analyze the performance of this message-passing algorithm, we borrow techniques from statistical physics and coding theory such as phase transition, correlation decay, and density evolution. Precisely, we show that above a phase transition, the graphical model exhibits correlation decay property. Then, an analysis technique known as density evolution gives a precise description of the density (or distribution) of the messages. Time permitting, I will discuss an interesting connection between this message-passing algorithm and the singular vectors of sparse random matrices.</p>
</div></div>
<p><a name="Onak"> </a></p>
<div class="infoblock">
<div class="blockcontent">
<p><i>TBA</i> <br />
<b>Krzysztof Onak</b> (IBM T. J. Watson)</p>
</div></div>
<p><a name="Pfister"> </a></p>
<div class="infoblock">
<div class="blockcontent">
<p><i>Factor Graphs, Belief Propagation, and Density Evolution</i> <br /> <b>Henry Pfister</b> (Duke University)<br /><br /> Abstract: The goal of this mini-course is to introduce students to marginal inference techniques for large systems of random variables defined by sparse random factor graphs.  Over the past 20 years, these techniques have revolutionized error-correcting codes, compressed sensing, and random satisfiability.  In particular, we consider approximate marginal inference based on the low-complexity iterative algorithm called belief propagation (BP).  In general, this algorithm is quite effective when the neighborhoods of most variable nodes do not contain short cycles.  Density evolution is a technique that, in some cases, allows one to rigorously analyze the asymptotic performance of BP as the size of the sparse random graph increases.  Each technique will be illustrated via worked examples and descriptions of how they are used in practice.</p>
</div></div>
<p><a name="Reeves"> </a></p>
<div class="infoblock">
<div class="blockcontent">
<p><i>Understanding Phase Transitions in Compressed Sensing </i> <br /> <b>Galen Reeves</b> (Duke University)<br /><br /> Abstract: Large compressed sensing problems can exhibit phase transitions in which a small change in the number of measurements leads to a large change in the mean-squared error. Over the past decade, these phase transitions have been studied using an amazingly diverse set of ideas from information theory, statistical physics, high-dimensional geometry, and statistical decision theory. The goal of this talk is to use an information theoretic framework to explain the connections between three very different methods of analysis.  The first uses the heuristic replica method from statistical physics to characterize the fundamental limits. The second uses the analysis of approximate loopy belief propagation to characterize the asymptotic performance of practical algorithms, and the third uses Gaussian process theory and concentration of measure to provide sharp non-asymptotic bounds for optimization-based algorithms.</p>
</div></div>
<p><a name="Saligrama"> </a></p>
<div class="infoblock">
<div class="blockcontent">
<p><i>A Geometric Approach to Learning Mixture-Models</i> <br /> <b>Venkatesh Saligrama</b> (Boston University)<br /><br /> Abstract: In a wide spectrum of problems in science and engineering that includes hyperspectral imaging, gene expression analysis, and metabolic networks, the observed data is high-dimensional and can be modeled as arising from an unknown mixture of a small set of unknown shared latent factors. Our approach is based on a natural separability property of the shared latent factors. Our separability property posits that every latent factor contains at least one component that is dominant in that factor. We first establish that this property is not only natural but an inevitable consequence of high-dimensionality, and satisfied by the estimates produced by popular nonparametric Bayes approaches. We show that geometrically these dominant latent factors can be associated with extreme points in a suitable space. We leverage this geometric insight  to develop a suite of efficient algorithms for a diverse set of latent variable problems. The proposed random-projections-based algorithm is naturally amenable to a low communication-cost distributed implementation that is attractive for modern web-scale distributed data mining applications. We then establish statistical and computational efficiency guarantees for learning in high-dimensional latent variable models.</p>
</div></div>
<p><a name="Shah"> </a></p>
<div class="infoblock">
<div class="blockcontent">
<p><i>Short Course on Graphical Models and Inference Algorithm</i> <br /> <b>Devavrat Shah</b> (MIT)<br /><br /> Abstract: In the statistical decision theory, decision making using data involves two key steps. First, learn &ldquo;model&rdquo; from historical observations, and Second, using thus learnt model along with new observations to make decision. The choice of model plays crucial role in terms of ability to perform good decisions and at scale. In a nutshell, complexity of model can help or hurt learning statistically and computationally. In the recent times, graphical models have emerged as an excellent candidate for learning structural models from data both statistically and computationally efficiently. </p>
<p>The purpose of this short course is to introduce graphical models, describe the inference algorithms associated with them as well as discuss how to learn them from data. We will reference various applications that utilize graphical models, but the content will be focused on graphical models rather than applications themselves. </p>
<p>No specific domain background will be assumed other than familiarity with basic probability.</p>
</div></div>
<p><a name="Vassilvitskii"> </a></p>
<div class="infoblock">
<div class="blockcontent">
<p><i>TBA</i> <br />
<b>Sergei Vassilvitskii</b> (Google)</p>
</div></div>
<p><a name="Woodruff"> </a></p>
<div class="infoblock">
<div class="blockcontent">
<p><i>TBA</i> <br />
<b>David Woodruff</b> (IBM Almaden)</p>
</div></div>
<p><a name="Wootters"> </a></p>
<div class="infoblock">
<div class="blockcontent">
<p><i>Learning polynomials from partial evaluations, and repairing Reed-Solomon codes</i> <br /> <b>Mary Wootters</b> (Carnegie Mellon University)<br /><br /> Abstract: A fundamental fact about polynomial interpolation is that \(k\) evaluations of a degree-\((k-1)\) polynomial \(f(x)\) are sufficient to determine \(f(x)\).  This is also necessary in a strong sense: given \(k-1\) evaluations of \(f\), we learn nothing about \(f(a)\) for any \(k\)th point a.  We study a variant of this polynomial interpolation problem.  Instead of getting complete evaluations of \(f(x)\) &ndash; which may live in a large finite field \(F\) &ndash; we are allowed to ask for <i>partial</i> evaluations &ndash; that is, a few symbols from a subfield of \(F\), rather than one symbol from \(F\) itself &ndash; from the evaluations \(f(x)\).  The goal is again to determine \(f(a)\) for some point a that was not queried, while minimizing the total amount of information gathered from the evaluations.</p>
<p>We show that in this model, one can do significantly better than the traditional setting, in terms of the amount of information required to determine \(f(a)\).  Our motivation comes from the use of Reed-Solomon codes in distributed storage systems, and as a corollary we give improved (and, in some parameter regimes, optimal) exact repair schemes for RS codes.</p>
<p>This is joint work with Venkat Guruswami.</p>
</div></div>
<div id="footer">
<div id="footer-text">
Page generated 2015-12-23 19:12:55 CST, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</td>
</tr>
</table>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-62001519-1', 'auto');
  ga('send', 'pageview');
</script>
</body>
</html>
